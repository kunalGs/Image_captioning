{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Mini_project_VR_1_with_Attention.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        " # First we import all the necessary packages"
      ],
      "metadata": {
        "id": "53PBXV6Jp0Zp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os \n",
        "import pandas as pd \n",
        "import spacy  # for tokenizer\n",
        "import torch\n",
        "from torch.nn.utils.rnn import pad_sequence  # pad batch\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from PIL import Image  # Load img\n",
        "import torchvision.transforms as transforms\n",
        "from tqdm import tqdm\n",
        "import torch.optim as optim\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "import sys\n",
        "import numpy as np\n",
        "import torch.nn.functional as F"
      ],
      "metadata": {
        "id": "x_UiUkhXp3WV"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def save_checkpoint(state, filename=\"my_checkpoint.pth.tar\"):\n",
        "    print(\"=> Saving checkpoint\")\n",
        "    torch.save(state, filename)\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ob8LURR_qar6"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_checkpoint(checkpoint, model, optimizer):\n",
        "    print(\"=> Loading checkpoint\")\n",
        "    model.load_state_dict(checkpoint[\"state_dict\"])\n",
        "    optimizer.load_state_dict(checkpoint[\"optimizer\"])\n",
        "    step = checkpoint[\"step\"]\n",
        "    return step\n"
      ],
      "metadata": {
        "id": "O56Bk0huAo66"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "RESIZE = 356\n",
        "CROP = 299"
      ],
      "metadata": {
        "id": "gZc05p1dAuoc"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.backends.cudnn.benchmark = True\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "id": "2WBEEeWtA2jC"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# We want to convert text -> numerical values\n",
        "# 1. We need a Vocabulary mapping each word to a index\n",
        "# 2. We need to setup a Pytorch dataset to load the data\n",
        "# 3. Setup padding of every batch (all examples should be of same seq_len and setup dataloader)\n",
        "# Note that loading the image is very easy compared to the text!\n",
        "# Download with: python -m spacy download en\n",
        "\n",
        "\n",
        "spacy_eng = spacy.load(\"en\")"
      ],
      "metadata": {
        "id": "z9HBGcCgBCBu"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def getNumberOfParameter(model):\n",
        "    print('Number of trainable params: ', sum(p.numel() for p in model.parameters() if p.requires_grad))\n",
        "    print('Total params: ', sum(p.numel() for p in model.parameters()))\n"
      ],
      "metadata": {
        "id": "6LJkvWACBv26"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Vocabulary:\n",
        "    def __init__(self, freq_threshold):\n",
        "        self.itos = {0: \"<PAD>\", 1: \"<SOS>\", 2: \"<EOS>\", 3: \"<UNK>\"}\n",
        "        self.stoi = {\"<PAD>\": 0, \"<SOS>\": 1, \"<EOS>\": 2, \"<UNK>\": 3}\n",
        "        self.freq_threshold = freq_threshold\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.itos)\n",
        "\n",
        "    @staticmethod\n",
        "    def tokenizer_eng(text):\n",
        "        return [tok.text.lower() for tok in spacy_eng.tokenizer(text)]\n",
        "\n",
        "    def build_vocabulary(self, sentence_list):\n",
        "        frequencies = {}\n",
        "        idx = 4\n",
        "\n",
        "        for sentence in sentence_list:\n",
        "            for word in self.tokenizer_eng(sentence):\n",
        "                if word not in frequencies:\n",
        "                    frequencies[word] = 1\n",
        "\n",
        "                else:\n",
        "                    frequencies[word] += 1\n",
        "\n",
        "                if frequencies[word] == self.freq_threshold:\n",
        "                    self.stoi[word] = idx\n",
        "                    self.itos[idx] = word\n",
        "                    idx += 1\n",
        "\n",
        "    def numericalize(self, text):\n",
        "        tokenized_text = self.tokenizer_eng(text)\n",
        "\n",
        "        return [\n",
        "            self.stoi[token] if token in self.stoi else self.stoi[\"<UNK>\"]\n",
        "            for token in tokenized_text\n",
        "        ]\n",
        "\n"
      ],
      "metadata": {
        "id": "Vr23Lt4ZCL9f"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class FlickrDataset(Dataset):\n",
        "    def __init__(self, root_dir, captions_file, choose_file, isTrain, transform=None, freq_threshold=2):\n",
        "        self.root_dir = root_dir\n",
        "        \n",
        "        self.df = pd.read_csv(captions_file, sep='\\t', names=['image', 'caption'])\n",
        "        self.df['image'] = self.df[\"image\"].str.split('#', 1, expand=True)[0]\n",
        "        \n",
        "        # Initialize vocabulary and build vocab\n",
        "        self.vocab = Vocabulary(freq_threshold)\n",
        "        self.vocab.build_vocabulary(self.df[\"caption\"].tolist())\n",
        "        \n",
        "        #self.df = self.df.groupby('image').first().reset_index()\n",
        "        \n",
        "        self.choose_df = pd.read_csv(choose_file, names=['image'])\n",
        "        if isTrain:\n",
        "            validation_df = pd.read_csv('/content/drive/MyDrive/VR_Image_captioning_project/Flickr8k_text/Flickr_8k.devImages.txt',names=['image'])\n",
        "            self.choose_df = pd.concat([self.choose_df, validation_df]).reset_index()\n",
        "        \n",
        "        self.choose_df = self.choose_df[:300]\n",
        "            \n",
        "        self.df = self.df.loc[self.df['image'].isin(self.choose_df['image'].values)].reset_index(drop=True)\n",
        "        \n",
        "        self.transform = transform\n",
        "\n",
        "        # Get img, caption columns\n",
        "        self.imgs = self.df[\"image\"]\n",
        "        self.captions = self.df[\"caption\"]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        caption = self.captions[index]\n",
        "        img_id = self.imgs[index]\n",
        "        img = Image.new('RGB', (RESIZE,RESIZE))\n",
        "        try:\n",
        "            img = Image.open(os.path.join(self.root_dir, img_id)).convert(\"RGB\")\n",
        "        except:\n",
        "            print(img_id)\n",
        "\n",
        "        if self.transform is not None:\n",
        "            img = self.transform(img)\n",
        "\n",
        "        numericalized_caption = [self.vocab.stoi[\"<SOS>\"]]\n",
        "        numericalized_caption += self.vocab.numericalize(caption)\n",
        "        numericalized_caption.append(self.vocab.stoi[\"<EOS>\"])\n",
        "\n",
        "        return img, torch.tensor(numericalized_caption)"
      ],
      "metadata": {
        "id": "dfkdXW0AD0EI"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MyCollate:\n",
        "    def __init__(self, pad_idx):\n",
        "        self.pad_idx = pad_idx\n",
        "\n",
        "    def __call__(self, batch):\n",
        "        imgs = [item[0].unsqueeze(0) for item in batch]\n",
        "        imgs = torch.cat(imgs, dim=0)\n",
        "        targets = [item[1] for item in batch]\n",
        "        targets = pad_sequence(targets, batch_first=False, padding_value=self.pad_idx)\n",
        "        \n",
        "        return imgs, targets"
      ],
      "metadata": {
        "id": "Qig8lhHEEsac"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_loader(\n",
        "    root_folder,\n",
        "    annotation_file,\n",
        "    choose_file,\n",
        "    transform,\n",
        "    batch_size=32,\n",
        "    num_workers=8,\n",
        "    shuffle=True,\n",
        "    pin_memory=True,\n",
        "    isTrain=True\n",
        "):\n",
        "    dataset = FlickrDataset(root_folder, annotation_file, choose_file, transform=transform, isTrain=isTrain)\n",
        "\n",
        "    pad_idx = dataset.vocab.stoi[\"<PAD>\"]\n",
        "\n",
        "    loader = DataLoader(\n",
        "        dataset=dataset,\n",
        "        batch_size=batch_size,\n",
        "        num_workers=num_workers,\n",
        "        shuffle=shuffle,\n",
        "        pin_memory=pin_memory,\n",
        "        collate_fn=MyCollate(pad_idx=pad_idx),\n",
        "    )\n",
        "\n",
        "    return loader, dataset"
      ],
      "metadata": {
        "id": "FnGYSqQeE4By"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Implement CNN and RNN\n",
        "import torch.nn as nn\n",
        "import statistics\n",
        "import torchvision.models as models"
      ],
      "metadata": {
        "id": "qz3_hpeEFCBe"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class EncoderCNN(nn.Module):\n",
        "    def __init__(self, train_CNN, encoded_image_size=14):\n",
        "        super(EncoderCNN, self).__init__()\n",
        "        self.train_CNN = train_CNN\n",
        "        self.encoded_image_size = encoded_image_size\n",
        "        \n",
        "        resnet = models.resnet18(pretrained=True)  #######  RESNET152 \n",
        "\n",
        "\n",
        "        modules = list(resnet.children())[:-2]\n",
        "        self.resnet = nn.Sequential(*modules)\n",
        "        self.adaptive_pool = nn.AdaptiveAvgPool2d((encoded_image_size, encoded_image_size))\n",
        "        \n",
        "        # Fine tune\n",
        "        for param in self.resnet.parameters():\n",
        "            param.requires_grad = False\n",
        "        \n",
        "        # Only train last 2 layers of resnet if at all required\n",
        "        if train_CNN:\n",
        "            for c in list(self.resnet.children())[-2:]:\n",
        "                for p in c.parameters():\n",
        "                    p.requires_grad = train_CNN\n",
        "        \n",
        "\n",
        "    def forward(self, images):\n",
        "        features = self.resnet(images) \n",
        "        features = self.adaptive_pool(features) # batch, 512, encoded_image_size, encoded_image_size\n",
        "        features = features.permute(0, 2, 3, 1) # batch, encoded_image_size, encoded_image_size, 512\n",
        "        return features"
      ],
      "metadata": {
        "id": "ASLqqPZNFIIK"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Attention(nn.Module):\n",
        "    def __init__(self, encoder_dim, decoder_dim, attention_dim):\n",
        "        super(Attention, self).__init__()\n",
        "        self.encoder_att = nn.Linear(encoder_dim, attention_dim)  \n",
        "        self.decoder_att = nn.Linear(decoder_dim, attention_dim)\n",
        "        self.full_att = nn.Linear(attention_dim, 1)  # linear layer to calculate values to be softmax-ed\n",
        "        self.relu = nn.ReLU()\n",
        "        self.softmax = nn.Softmax(dim=1) \n",
        "\n",
        "    def forward(self, encoder_out, decoder_hidden):\n",
        "        att1 = self.encoder_att(encoder_out)\n",
        "        att2 = self.decoder_att(decoder_hidden)\n",
        "        att = self.full_att(self.relu(att1 + att2.unsqueeze(1))).squeeze(2)  # (batch_size, num_pixels)\n",
        "        alpha = self.softmax(att)  # (batch_size, num_pixels)\n",
        "        attention_weighted_encoding = (encoder_out * alpha.unsqueeze(2)).sum(dim=1)  # (batch_size, encoder_dim)\n",
        "\n",
        "        return attention_weighted_encoding, alpha"
      ],
      "metadata": {
        "id": "spLo7TyrFO7J"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DecoderRNN(nn.Module):\n",
        "    def __init__(self, attention_dim, embed_dim, decoder_dim, vocab_size, encoder_dim, dropout, teacher):\n",
        "        \"\"\"\n",
        "        decoder_dim is hidden_size for lstm cell\n",
        "        \"\"\"\n",
        "        super(DecoderRNN, self).__init__()\n",
        "        self.encoder_dim = encoder_dim\n",
        "        self.attention_dim = attention_dim\n",
        "        self.embed_dim = embed_dim\n",
        "        self.decoder_dim = decoder_dim\n",
        "        self.vocab_size = vocab_size\n",
        "        self.dropout = dropout\n",
        "        self.teacher = teacher\n",
        "        \n",
        "        self.attention = Attention(encoder_dim, decoder_dim, attention_dim)  \n",
        "        \n",
        "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
        "        self.dropout = nn.Dropout(p=self.dropout)\n",
        "        self.decode_step = nn.LSTMCell(embed_dim + encoder_dim, decoder_dim, bias=True)\n",
        "        self.init_h = nn.Linear(encoder_dim, decoder_dim)  # linear layer to find initial hidden state of LSTMCell\n",
        "        self.init_c = nn.Linear(encoder_dim, decoder_dim)  # linear layer to find initial cell state of LSTMCell\n",
        "        self.f_beta = nn.Linear(decoder_dim, encoder_dim)  # linear layer to create a sigmoid-activated gate\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "        self.fc = nn.Linear(decoder_dim, vocab_size)  # linear layer to find scores over vocabulary\n",
        "        self.init_weights()  # initialize some layers with the uniform distribution\n",
        "        \n",
        "    def init_weights(self):\n",
        "        \"\"\"\n",
        "        Initializes some parameters with values from the uniform distribution, for easier convergence.\n",
        "        \"\"\"\n",
        "        self.embedding.weight.data.uniform_(-0.1, 0.1)\n",
        "        self.fc.bias.data.fill_(0)\n",
        "        self.fc.weight.data.uniform_(-0.1, 0.1)\n",
        "    \n",
        "    def init_hidden_state(self, encoder_out):\n",
        "        \"\"\"\n",
        "        Creates the initial hidden and cell states for the decoder's LSTM based on the encoded images.\n",
        "        :param encoder_out: encoded images, a tensor of dimension (batch_size, num_pixels, encoder_dim)\n",
        "        :return: hidden state, cell state\n",
        "        \"\"\"\n",
        "        mean_encoder_out = encoder_out.mean(dim=1)\n",
        "        h = self.init_h(mean_encoder_out)  # (batch_size, decoder_dim)\n",
        "        c = self.init_c(mean_encoder_out)\n",
        "        return h, c\n",
        "\n",
        "    def forward(self, encoder_out, encoded_captions):\n",
        "        \"\"\"\n",
        "        :param encoder_out: encoded images, a tensor of dimension (batch_size, enc_image_size, enc_image_size, encoder_dim)\n",
        "        :param encoded_captions: encoded captions, a tensor of dimension (batch_size, max_caption_length)\n",
        "        :return: scores for vocabulary, sorted encoded captions, decode lengths, weights, sort indices\n",
        "        \"\"\"\n",
        "        \n",
        "        batch_size = encoder_out.size(0)\n",
        "        encoder_dim = encoder_out.size(-1)\n",
        "        vocab_size = self.vocab_size\n",
        "        \n",
        "        # Flatten image\n",
        "        encoder_out = encoder_out.view(batch_size, -1, encoder_dim)  # (batch_size, num_pixels, encoder_dim)\n",
        "        num_pixels = encoder_out.size(1)\n",
        "        \n",
        "        # Embedding\n",
        "        if teacher:\n",
        "            embeddings = self.embedding(encoded_captions)  # (batch_size, max_caption_length, embed_dim)\n",
        "        else:\n",
        "            preds = torch.ones((batch_size, 1), dtype=torch.int64).to(device)\n",
        "                \n",
        "        h, c = self.init_hidden_state(encoder_out)  # (batch_size, decoder_dim)\n",
        "        \n",
        "        decode_length = encoded_captions.size(1)-1\n",
        "        \n",
        "        # Create tensors to hold word predicion scores and alphas\n",
        "        predictions = torch.zeros(batch_size, decode_length, vocab_size).to(device)\n",
        "        alphas = torch.zeros(batch_size, decode_length, num_pixels).to(device)\n",
        "        \n",
        "        for t in range(decode_length):\n",
        "            if teacher:\n",
        "                inputEmbeddings = embeddings[:, t, :]\n",
        "            else:\n",
        "                inputEmbeddings = self.embedding(preds)[:, 0, :]\n",
        "            \n",
        "            attention_weighted_encoding, alpha = self.attention(encoder_out, h)\n",
        "            gate = self.sigmoid(self.f_beta(h))  # gating scalar, (batch_size_t, encoder_dim)\n",
        "            attention_weighted_encoding = gate * attention_weighted_encoding\n",
        "            \n",
        "            h, c = self.decode_step(torch.cat([inputEmbeddings, attention_weighted_encoding], dim=1), (h, c))  #(batch_size_t, decoder_dim)\n",
        "            preds = self.fc(self.dropout(h))  # (batch_size_t, vocab_size)\n",
        "            predictions[:, t, :] = preds\n",
        "            preds = preds.argmax(1).unsqueeze(1)\n",
        "            alphas[:, t, :] = alpha\n",
        "\n",
        "        return predictions, alphas"
      ],
      "metadata": {
        "id": "48PqLbKEFpkY"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CNNtoRNN(nn.Module):\n",
        "    def __init__(self, attention_dim, embed_dim, decoder_dim, vocab_size,\n",
        "                 encoder_dim=512, dropout=0.5, train_CNN=False, teacher=True):\n",
        "        super(CNNtoRNN, self).__init__()\n",
        "        self.encoderCNN = EncoderCNN(train_CNN=train_CNN)\n",
        "        self.decoderRNN = DecoderRNN(attention_dim, embed_dim, decoder_dim, vocab_size,\n",
        "                                     encoder_dim=encoder_dim, dropout=dropout, teacher=teacher)\n",
        "\n",
        "    def forward(self, images, captions):\n",
        "        encoder_out = self.encoderCNN(images)\n",
        "        outputs, alphas = self.decoderRNN(encoder_out, captions)\n",
        "        return outputs, alphas\n",
        "\n",
        "    def caption_image(self, image, vocabulary, max_length=50):\n",
        "        result_caption = [1]\n",
        "    \n",
        "        with torch.no_grad():\n",
        "            encoder_out = self.encoderCNN(image)\n",
        "            \n",
        "            batch_size = encoder_out.size(0)\n",
        "            encoder_dim = encoder_out.size(-1)\n",
        "            vocab_size = self.decoderRNN.vocab_size\n",
        "            \n",
        "            encoder_out = encoder_out.view(batch_size, -1, encoder_dim)  # (batch_size, num_pixels, encoder_dim)\n",
        "            num_pixels = encoder_out.size(1)\n",
        "            \n",
        "            # initially start with sos as a predicted word\n",
        "            predicted = torch.tensor([vocabulary.stoi[\"<SOS>\"]]).to(device)\n",
        "            h, c = self.decoderRNN.init_hidden_state(encoder_out)  # (batch_size, decoder_dim)\n",
        "            \n",
        "            for t in range(max_length):\n",
        "                embeddings = self.decoderRNN.embedding(predicted)  # (1, embed_dim)\n",
        "                \n",
        "                attention_weighted_encoding, alpha = self.decoderRNN.attention(encoder_out, h)\n",
        "                gate = self.decoderRNN.sigmoid(self.decoderRNN.f_beta(h))  # gating scalar, (batch_size_t, encoder_dim)\n",
        "                attention_weighted_encoding = gate * attention_weighted_encoding\n",
        "                \n",
        "                h, c = self.decoderRNN.decode_step(torch.cat([embeddings, attention_weighted_encoding], dim=1), (h, c))  #(batch_size_t, decoder_dim)\n",
        "                preds = self.decoderRNN.fc(self.decoderRNN.dropout(h))  # (batch_size_t, vocab_size)\n",
        "                    \n",
        "                predicted = preds.argmax(1)\n",
        "                result_caption.append(predicted.item())\n",
        "                \n",
        "                if vocabulary.itos[predicted.item()] == \"<EOS>\":\n",
        "                    break\n",
        "            \n",
        "            return [vocabulary.itos[idx] for idx in result_caption]"
      ],
      "metadata": {
        "id": "QmaEN9vpGFRd"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the model\n",
        "batch_size=8\n",
        "transform = transforms.Compose(\n",
        "    [\n",
        "        transforms.Resize((RESIZE, RESIZE)),\n",
        "        transforms.RandomCrop((CROP, CROP)),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.ColorJitter(),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
        "    ]\n",
        ")\n",
        "train_loader, dataset = get_loader(\n",
        "    '/content/drive/MyDrive/VR_Image_captioning_project/Flicker8k_Dataset',\n",
        "    '/content/drive/MyDrive/VR_Image_captioning_project/Flickr8k_text/Flickr8k.token.txt',\n",
        "    '/content/drive/MyDrive/VR_Image_captioning_project/Flickr8k_text/Flickr_8k.trainImages.txt',\n",
        "    transform=transform,\n",
        "    num_workers=8,\n",
        "    batch_size=batch_size,\n",
        "    shuffle=True,\n",
        "    isTrain=True\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XKMglhAgGGfY",
        "outputId": "d78bf280-693d-41fa-c566-71c977cb10d6"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:490: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "ka4oyp7AndQM"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "451PLIVwkdNK",
        "outputId": "814d612b-25ba-4937-82e9-ad80c4a8653a"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Hyperparameters\n",
        "attention_dim = 512\n",
        "embed_dim = 512\n",
        "decoder_dim = 512\n",
        "dropout = 0.5\n",
        "vocab_size = len(dataset.vocab)\n",
        "learning_rate = 1e-03\n",
        "num_epochs = 20\n",
        "load_model = True\n",
        "save_model = True\n",
        "train_CNN = False\n",
        "alpha_c = 1\n",
        "teacher = False\n",
        "\n",
        "def train():\n",
        "\n",
        "    #for tensorboard:\n",
        "    # writer = SummaryWriter(\"runs/flickr\")\n",
        "    step = 0\n",
        "\n",
        "    # initialize model, loss etc\n",
        "    model = CNNtoRNN(attention_dim, embed_dim, decoder_dim, vocab_size,\n",
        "                     train_CNN=train_CNN, dropout=dropout, teacher=teacher).to(device)\n",
        "    criterion = nn.CrossEntropyLoss(ignore_index=dataset.vocab.stoi[\"<PAD>\"])\n",
        "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "\n",
        "    # if load_model:\n",
        "    #     step = load_checkpoint(torch.load(\"my_checkpoint.pth.tar\"), model, optimizer)\n",
        "    #     #step = load_checkpoint(torch.load(\"../input/flickr8k/my_checkpoint.pth.tar\"), model, optimizer)\n",
        "\n",
        "    model.train()\n",
        "  \n",
        "    for epoch in range(num_epochs):\n",
        "        \n",
        "        loss = 100\n",
        "        if save_model:\n",
        "            checkpoint = {\n",
        "                \"state_dict\": model.state_dict(),\n",
        "                \"optimizer\": optimizer.state_dict(),\n",
        "                \"step\": step,\n",
        "            }\n",
        "            save_checkpoint(checkpoint)\n",
        "            torch.save(model.state_dict(), 'puremodel.pth.tar')\n",
        "\n",
        "        for idx, (imgs, captions) in tqdm(enumerate(train_loader), total=len(train_loader), leave=False):\n",
        "            imgs = imgs.to(device)\n",
        "            captions = captions.to(device)\n",
        "\n",
        "            outputs, alphas = model(imgs, captions.permute(1, 0))\n",
        "            loss = criterion(outputs.reshape(-1, outputs.shape[2]), captions.permute(1, 0)[:, 1:].reshape(-1))\n",
        "\n",
        "            # writer.add_scalar(\"Training loss\", loss.item(), global_step=step)\n",
        "            step += 1\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward(loss)\n",
        "            \n",
        "            #Add doubly stochastic attention regularization\n",
        "            loss += alpha_c * ((1. - alphas.sum(dim=1)) ** 2).mean()\n",
        "            \n",
        "            optimizer.step()\n",
        "        \n",
        "        print('Epoch {} completed with loss {}'.format(epoch+1, loss))\n",
        "\n",
        "\n",
        "#train()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hrGvz_q4G0Dv",
        "outputId": "17274f09-65aa-407a-f293-52eb8a46987b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=> Saving checkpoint\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:490: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 completed with loss 6.215246200561523\n",
            "=> Saving checkpoint\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ""
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2 completed with loss 5.46951150894165\n",
            "=> Saving checkpoint\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ""
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3 completed with loss 6.265564918518066\n",
            "=> Saving checkpoint\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ""
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4 completed with loss 6.098518371582031\n",
            "=> Saving checkpoint\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ""
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5 completed with loss 5.322718620300293\n",
            "=> Saving checkpoint\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ""
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 6 completed with loss 5.910269737243652\n",
            "=> Saving checkpoint\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ""
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 7 completed with loss 5.326091289520264\n",
            "=> Saving checkpoint\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ""
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 8 completed with loss 4.898952484130859\n",
            "=> Saving checkpoint\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ""
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 9 completed with loss 4.730189800262451\n",
            "=> Saving checkpoint\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ""
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 10 completed with loss 5.298147678375244\n",
            "=> Saving checkpoint\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ""
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 11 completed with loss 5.2580790519714355\n",
            "=> Saving checkpoint\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ""
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 12 completed with loss 5.1921563148498535\n",
            "=> Saving checkpoint\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ""
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 13 completed with loss 4.418233871459961\n",
            "=> Saving checkpoint\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ""
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 14 completed with loss 4.568251609802246\n",
            "=> Saving checkpoint\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ""
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 15 completed with loss 4.581398963928223\n",
            "=> Saving checkpoint\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ""
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 16 completed with loss 3.9175100326538086\n",
            "=> Saving checkpoint\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ""
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 17 completed with loss 4.495271682739258\n",
            "=> Saving checkpoint\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ""
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 18 completed with loss 4.820018291473389\n",
            "=> Saving checkpoint\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ""
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 19 completed with loss 4.802123546600342\n",
            "=> Saving checkpoint\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ""
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 20 completed with loss 4.483929634094238\n",
            "=> Saving checkpoint\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ""
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 21 completed with loss 4.255615234375\n",
            "=> Saving checkpoint\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ""
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 22 completed with loss 4.535081386566162\n",
            "=> Saving checkpoint\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ""
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 23 completed with loss 4.684027671813965\n",
            "=> Saving checkpoint\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ""
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 24 completed with loss 4.00667667388916\n",
            "=> Saving checkpoint\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ""
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 25 completed with loss 4.269856929779053\n",
            "=> Saving checkpoint\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ""
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 26 completed with loss 4.029437065124512\n",
            "=> Saving checkpoint\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ""
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 27 completed with loss 4.248364448547363\n",
            "=> Saving checkpoint\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ""
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 28 completed with loss 4.31568717956543\n",
            "=> Saving checkpoint\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ""
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 29 completed with loss 4.310994625091553\n",
            "=> Saving checkpoint\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ""
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 30 completed with loss 4.017230987548828\n",
            "=> Saving checkpoint\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ""
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 31 completed with loss 4.542737007141113\n",
            "=> Saving checkpoint\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ""
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 32 completed with loss 4.422666072845459\n",
            "=> Saving checkpoint\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ""
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 33 completed with loss 3.635453224182129\n",
            "=> Saving checkpoint\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ""
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 34 completed with loss 4.14889669418335\n",
            "=> Saving checkpoint\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ""
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 35 completed with loss 3.879477024078369\n",
            "=> Saving checkpoint\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ""
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 36 completed with loss 3.4731693267822266\n",
            "=> Saving checkpoint\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ""
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 37 completed with loss 4.567559719085693\n",
            "=> Saving checkpoint\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ""
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 38 completed with loss 3.5936696529388428\n",
            "=> Saving checkpoint\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ""
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 39 completed with loss 3.9755353927612305\n",
            "=> Saving checkpoint\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ""
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 40 completed with loss 4.354511737823486\n",
            "=> Saving checkpoint\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ""
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 41 completed with loss 3.6624350547790527\n",
            "=> Saving checkpoint\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ""
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 42 completed with loss 4.159901142120361\n",
            "=> Saving checkpoint\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ""
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 43 completed with loss 3.413750410079956\n",
            "=> Saving checkpoint\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ""
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 44 completed with loss 4.063028335571289\n",
            "=> Saving checkpoint\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ""
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 45 completed with loss 3.629020929336548\n",
            "=> Saving checkpoint\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ""
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 46 completed with loss 3.5833587646484375\n",
            "=> Saving checkpoint\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ""
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 47 completed with loss 3.5134713649749756\n",
            "=> Saving checkpoint\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ""
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 48 completed with loss 3.7678093910217285\n",
            "=> Saving checkpoint\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ""
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 49 completed with loss 3.8902626037597656\n",
            "=> Saving checkpoint\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ""
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 50 completed with loss 4.551187038421631\n",
            "=> Saving checkpoint\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ""
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 51 completed with loss 3.7326149940490723\n",
            "=> Saving checkpoint\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ""
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 52 completed with loss 3.8528380393981934\n",
            "=> Saving checkpoint\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ""
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 53 completed with loss 3.284317970275879\n",
            "=> Saving checkpoint\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ""
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 54 completed with loss 4.044513702392578\n",
            "=> Saving checkpoint\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ""
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 55 completed with loss 3.680896282196045\n",
            "=> Saving checkpoint\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ""
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 56 completed with loss 3.7453413009643555\n",
            "=> Saving checkpoint\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ""
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 57 completed with loss 3.529026508331299\n",
            "=> Saving checkpoint\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ""
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 58 completed with loss 3.239643096923828\n",
            "=> Saving checkpoint\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ""
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 59 completed with loss 3.8807196617126465\n",
            "=> Saving checkpoint\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ""
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 60 completed with loss 4.397519588470459\n",
            "=> Saving checkpoint\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ""
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 61 completed with loss 3.547095775604248\n",
            "=> Saving checkpoint\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ""
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 62 completed with loss 3.5882034301757812\n",
            "=> Saving checkpoint\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ""
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 63 completed with loss 4.112112045288086\n",
            "=> Saving checkpoint\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ""
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 64 completed with loss 3.9710707664489746\n",
            "=> Saving checkpoint\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ""
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 65 completed with loss 3.930950164794922\n",
            "=> Saving checkpoint\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ""
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 66 completed with loss 3.2590532302856445\n",
            "=> Saving checkpoint\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ""
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 67 completed with loss 3.529383420944214\n",
            "=> Saving checkpoint\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ""
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 68 completed with loss 4.019657611846924\n",
            "=> Saving checkpoint\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ""
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 69 completed with loss 4.313803195953369\n",
            "=> Saving checkpoint\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ""
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 70 completed with loss 3.3411049842834473\n",
            "=> Saving checkpoint\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ""
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 71 completed with loss 3.3341805934906006\n",
            "=> Saving checkpoint\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ""
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 72 completed with loss 3.7665181159973145\n",
            "=> Saving checkpoint\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ""
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 73 completed with loss 3.3552560806274414\n",
            "=> Saving checkpoint\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ""
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 74 completed with loss 3.7862091064453125\n",
            "=> Saving checkpoint\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 16%|█▌        | 30/188 [00:06<00:26,  5.99it/s]"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_loader, test_dataset = get_loader(\n",
        "    '/content/drive/MyDrive/VR_Image_captioning_project/Flicker8k_Dataset',\n",
        "    '/content/drive/MyDrive/VR_Image_captioning_project/Flickr8k_text/Flickr8k.token.txt',\n",
        "    '/content/drive/MyDrive/VR_Image_captioning_project/Flickr8k_text/Flickr_8k.testImages.txt',\n",
        "    transform=transform,\n",
        "    num_workers=8,\n",
        "    shuffle=False,\n",
        "    isTrain=False\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        },
        "id": "77VZPO9OHIlN",
        "outputId": "2716d6cd-058e-4de4-a775-3018f465dd6f"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-748842a39a9f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m test_loader, test_dataset = get_loader(\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0;34m'/content/drive/MyDrive/VR_Image_captioning_project/Flicker8k_Dataset'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;34m'/content/drive/MyDrive/VR_Image_captioning_project/Flickr8k_text/Flickr8k.token.txt'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;34m'/content/drive/MyDrive/VR_Image_captioning_project/Flickr8k_text/Flickr_8k.testImages.txt'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mtransform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'get_loader' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = CNNtoRNN(attention_dim, embed_dim, decoder_dim, vocab_size, train_CNN=train_CNN, dropout=dropout).to(device)\n",
        "model.load_state_dict(torch.load(\"./my_checkpoint.pth.tar\")['state_dict'])\n",
        "#model.load_state_dict(torch.load(\"../input/flickr8k/my_checkpoint.pth.tar\")['state_dict'])\n",
        "model.eval()\n",
        "\n",
        "predicted_captions = []\n",
        "i = 0\n",
        "for idx, (imgs, captions) in tqdm(\n",
        "            enumerate(test_loader), total=len(test_loader), leave=False\n",
        "        ):\n",
        "    for k in range(imgs.shape[0]):\n",
        "        img = imgs[k].unsqueeze(0)\n",
        "        real_caption = [dataset.vocab.itos[j.item()] for j in captions[:, k]]\n",
        "        predicted_captions.append([model.caption_image(img.to(device), dataset.vocab), real_caption])\n",
        "        i += 1\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "i = 0\n",
        "references_corpus = []\n",
        "candidate_corpus = []\n",
        "for e in predicted_captions:\n",
        "    if i % 5 == 0:\n",
        "        if i < 21:\n",
        "            print('Image name: {}'.format(test_dataset.df['image'][i]))\n",
        "            print('Real caption: ', [ e for e in predicted_captions[i][1] if e != '<PAD>'])\n",
        "            print('Predicted caption: ', [ e for e in predicted_captions[i][0] if e != '<PAD>'])\n",
        "            print('\\n')\n",
        "        references_corpus.append([[ e for e in predicted_captions[i][1] if e != '<PAD>']])\n",
        "        candidate_corpus.append([ e for e in predicted_captions[i][0] if e != '<PAD>'])\n",
        "        \n",
        "    else:\n",
        "        references_corpus[i//5].append([ e for e in predicted_captions[i][1] if e != '<PAD>'])\n",
        "        \n",
        "    i+=1"
      ],
      "metadata": {
        "id": "jRTEOUF7HdEV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torchtext.data.metrics import bleu_score\n",
        "print(bleu_score(candidate_corpus, references_corpus))"
      ],
      "metadata": {
        "id": "HOg-YG6iHrGm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "getNumberOfParameter(model)"
      ],
      "metadata": {
        "id": "Hwd9pEO5g59E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "base_path = '/content/drive/MyDrive/VR_Image_captioning_project/test_images/'\n",
        "def showAndCaptionImage(img, model):\n",
        "    \n",
        "    img = Image.open(base_path + img).convert(\"RGB\")\n",
        "    plt.imshow(img)\n",
        "    plt.show()\n",
        "    img = transform(img)\n",
        "    caption = model.caption_image(img.unsqueeze(0).to(device), dataset.vocab)[1:-1]\n",
        "    captionStr = \"\"\n",
        "    for e in caption:\n",
        "        captionStr += e + \" \"\n",
        "    print(captionStr)\n",
        "\n",
        "subjective_images = ['test_1.jpg','test_2.jpg','test_3.jpg','test_4.jpg','test_5.jpg']\n",
        "for image in subjective_images:\n",
        "    showAndCaptionImage(image, model)\n"
      ],
      "metadata": {
        "id": "ra-g2Wexgy6V"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}